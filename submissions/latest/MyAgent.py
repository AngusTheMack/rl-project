#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('AbstractAgent.py', 'class AbstractAgent:\n    """\n    AbstractAgent\n\n    """\n\n    def __init__(self):\n        raise NotImplementedError()\n\n    def act(self, observation):\n        raise NotImplementedError()\n')
    __stickytape_write_module('dqn/agent.py', 'import torch\nimport torch.nn.functional as F\nfrom gym import spaces\n\nfrom torch.optim import Optimizer\nimport numpy as np\n\nfrom dqn.model import DQN\nfrom dqn.replay_buffer import ReplayBuffer\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n\nclass DQNAgent:\n    def __init__(\n        self,\n        observation_space: spaces.Box,\n        action_space: spaces.Discrete,\n        replay_buffer: ReplayBuffer,\n        use_double_dqn,\n        lr,\n        batch_size,\n        gamma,\n    ):\n        """\n        Initialise the DQN algorithm using the Adam optimiser\n        :param action_space: the action space of the environment\n        :param observation_space: the state space of the environment\n        :param replay_buffer: storage for experience replay\n        :param lr: the learning rate for Adam\n        :param batch_size: the batch size\n        :param gamma: the discount factor\n        """\n        self.memory = replay_buffer\n        self.batch_size = batch_size\n        self.use_double_dqn = use_double_dqn\n        self.gamma = gamma\n        self.policy_network = DQN(observation_space, action_space).to(device)\n        self.target_network = DQN(observation_space, action_space).to(device)\n        self.update_target_network()\n        self.target_network.eval()\n        self.optimiser = torch.optim.Adam(self.policy_network.parameters(), lr=lr)\n\n    def optimise_td_loss(self):\n        """\n        Optimise the TD-error over a single minibatch of transitions\n        :return: the loss\n        """\n        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n        states = np.array(states) / 255.0\n        next_states = np.array(next_states) / 255.0\n        states = torch.from_numpy(states).float().to(device)\n        actions = torch.from_numpy(actions).long().to(device)\n        rewards = torch.from_numpy(rewards).float().to(device)\n        next_states = torch.from_numpy(next_states).float().to(device)\n        dones = torch.from_numpy(dones).float().to(device)\n\n        with torch.no_grad():\n            if self.use_double_dqn:\n                _, max_next_action = self.policy_network(next_states).max(1)\n                max_next_q_values = self.target_network(next_states).gather(1, max_next_action.unsqueeze(1)).squeeze()\n            else:\n                next_q_values = self.target_network(next_states)\n                max_next_q_values, _ = next_q_values.max(1)\n            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n\n        input_q_values = self.policy_network(states)\n        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n\n        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n\n        self.optimiser.zero_grad()\n        loss.backward()\n        self.optimiser.step()\n        del states\n        del next_states\n        return loss.item()\n\n    def update_target_network(self):\n        """\n        Update the target Q-network by copying the weights from the current Q-network\n        """\n        self.target_network.load_state_dict(self.policy_network.state_dict())\n\n    def act(self, state: np.ndarray):\n        """\n        Select an action greedily from the Q-network given the state\n        :param state: the current state\n        :return: the action to take\n        """\n        state = np.array(state) / 255.0\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        with torch.no_grad():\n            q_values = self.policy_network(state)\n            _, action = q_values.max(1)\n            return action.item()\n')
    __stickytape_write_module('dqn/model.py', 'import torch.nn as nn\nimport torch.nn.functional as F\nfrom gym import spaces\n\n\nclass DQN(nn.Module):\n    """\n    A basic implementation of a Deep Q-Network. The architecture is as described in the\n    Nature DQN paper.\n    """\n\n    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete):\n        """\n        Initialise the DQN\n        :param observation_space: the state space of the environment\n        :param action_space: the action space of the environment\n        """\n        super().__init__()\n        assert type(observation_space) == spaces.Box, "observation_space must be of type Box"\n        assert len(observation_space.shape) == 3, "observation space must have the form channels x width x height"\n        assert type(action_space) == spaces.Discrete, "action_space must be of type Discrete"\n\n        self.conv1 = nn.Conv2d(observation_space.shape[0], 32, 8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n        self.linear1 = nn.Linear(64 * 7 * 7, 512)\n        self.linear2 = nn.Linear(512, action_space.n)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(-1, 64 * 7 * 7)  # flatten\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n')
    __stickytape_write_module('dqn/replay_buffer.py', 'import numpy as np\n\n\nclass ReplayBuffer:\n    """\n    Simple storage for transitions from an environment.\n    """\n\n    def __init__(self, size):\n        """\n        Initialise a buffer of a given size for storing transitions\n        :param size: the maximum number of transitions that can be stored\n        """\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, state, action, reward, next_state, done):\n        """\n        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n        :param state: the agent\'s initial state\n        :param action: the action taken by the agent\n        :param reward: the reward the agent received\n        :param next_state: the subsequent state\n        :param done: whether the episode terminated\n        """\n        data = (state, action, reward, next_state, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, indices):\n        states, actions, rewards, next_states, dones = [], [], [], [], []\n        for i in indices:\n            data = self._storage[i]\n            state, action, reward, next_state, done = data\n            states.append(np.array(state, copy=False))\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(np.array(next_state, copy=False))\n            dones.append(done)\n        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n\n    def sample(self, batch_size):\n        """\n        Randomly sample a batch of transitions from the buffer.\n        :param batch_size: the number of transitions to sample\n        :return: a mini-batch of sampled transitions\n        """\n        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n        return self._encode_sample(indices)\n')
    __stickytape_write_module('dqn/wrappers.py', '"""\nUseful wrappers taken from OpenAI (https://github.com/openai/baselines)\n"""\n\nimport numpy as np\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\n\ncv2.ocl.setUseOpenCL(False)\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """ Do no-op action for a number of steps in [1, noop_max]."""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """Take action on reset for environments that are fixed until firing."""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """Return only every `skip`-th frame"""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, action):\n        """Repeat action, sum reward, and max over last observations."""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """Bin reward to {+1, 0, -1} by its sign."""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        """\n        Warp frames to 84x84 as done in the Nature paper and later work.\n        Expects inputs to be of shape height x width x num_channels\n        """\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = 84\n        self.height = 84\n        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n\n    def observation(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame[:, :, None]\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        Expects inputs to be of shape num_channels x height x width.\n        """\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers."""\n        self._frames = frames\n\n    def __array__(self, dtype=None):\n        out = np.concatenate(self._frames, axis=0)\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._frames)\n\n    def __getitem__(self, i):\n        return self._frames[i]\n\n\n\nclass PyTorchFrame(gym.ObservationWrapper):\n    """Image shape to num_channels x height x width"""\n\n    def __init__(self, env):\n        super(PyTorchFrame, self).__init__(env)\n        shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8)\n\n    def observation(self, observation):\n        return np.rollaxis(observation, 2)\n')
    from AbstractAgent import AbstractAgent
    
    from dqn.agent import DQNAgent
    from dqn.wrappers import *
    from dqn.replay_buffer import ReplayBuffer
    import torch
    device = torch.device("cpu")
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    class MyAgent(AbstractAgent):
        def __init__(self, observation_space, action_space):
            self.observation_space = observation_space
            self.action_space = action_space
            print(self.observation_space)
            print(self.action_space)
            # TODO Initialise your agent's models
            replay_buffer = ReplayBuffer(int(5e3))
            # for example, if your agent had a Pytorch model it must be load here
            # model.load_state_dict(torch.load( 'path_to_network_model_file', map_location=torch.device(device)))
            self.agent = DQNAgent(
                observation_space,
                action_space,
                replay_buffer,
                use_double_dqn=True,
                lr=1e-4,
                batch_size=32,
                gamma=0.99,
            )
            self.agent.policy_network.load_state_dict(torch.load("model.pth",map_location=torch.device(device)))
            # agent.policy_network.load_state_dict(torch.load(args.checkpoint))
        def act(self, observation):
            return self.agent.act(observation)
    